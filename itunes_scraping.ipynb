{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping iTunes podcast information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from string import ascii_uppercase\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape podcast genres from apple\n",
    "We first get the list of genres and the their links from the iTunes homepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse html from genre overview and extract the main genres\n",
    "page = requests.get('https://podcasts.apple.com/us/genre/podcasts/id26')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "main_category = soup.select('.top-level-genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we store the links in a dictionary using the genre name as key\n",
    "main_cat_dict = {}\n",
    "for item in main_category:\n",
    "    main_cat = item['title'].split(\" - \")[0]\n",
    "    link = item['href']\n",
    "    main_cat_dict[main_cat] = link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the genres for podcast links and id's\n",
    "Cool, we have now all the genres of the podcasts that are listed on the podcast page. We now want to traverse through the pages and collect the podcast links and their id to later use the links to gather additional information of every podcast. There are some duplicates in the id's we will investigate and resolve this later. For now we simply go through all the pages of a genre and a letter. If there are no more pages, the page will not show a 'next' link. We use this to break our loop and move on the next letter or genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_genre_dict = {}\n",
    "\n",
    "\n",
    "for key in main_cat_dict:\n",
    "    \n",
    "    podcast_link_list = []\n",
    "    podcast_id_list = []\n",
    "    \n",
    "    genre = main_cat_dict[key]\n",
    "\n",
    "    for letter in ascii_uppercase + \"*\":\n",
    "\n",
    "        page = 1\n",
    "        new_casts = True\n",
    "        \n",
    "        print(\"genre: \", key, \"letter :\", letter) \n",
    "\n",
    "        while new_casts == True:\n",
    "        \n",
    "            page_string = genre + \"?letter=\" +  letter + \"&page=\" + str(page)\n",
    "            subpage = requests.get(page_string)\n",
    "\n",
    "            soup_subpage = BeautifulSoup(subpage.content, 'html.parser')\n",
    "\n",
    "            podcasts = soup_subpage.select('#selectedcontent ul>li a')\n",
    "\n",
    "            for podcast in podcasts:\n",
    "\n",
    "                podcast_link = podcast['href']\n",
    "                # there are podcast with an empty title string\n",
    "                try:\n",
    "                    podcast_id = podcast['href'].split(\"/\")[6]\n",
    "                except:\n",
    "                    podcast_id = podcast['href'].split(\"/\")[5]\n",
    "                \n",
    "                podcast_id_list.append(podcast_id)\n",
    "                podcast_link_list.append(podcast_link)\n",
    "                \n",
    "            # check if there is a next link\n",
    "            next_link = soup_subpage.select('.paginate-more')\n",
    "                \n",
    "            # there is a next link on top and at the bottom of every page\n",
    "            if len(next_link) < 2:\n",
    "                #print(podcast_link)\n",
    "                new_casts = False\n",
    "\n",
    "                #page = page + 1\n",
    "                print(\"letter: \", letter, \"page :\", page)\n",
    "                break\n",
    "            page = page + 1\n",
    "            # store results for one genre in dictionary\n",
    "    podcast_genre_dict[key] = {'links':podcast_link_list , 'ids':podcast_id_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of podcasts across all genres\n",
    "genre_numbers = [len(podcast_genre_dict[key]['links']) for key in podcast_genre_dict]\n",
    "sum(genre_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with duplicates\n",
    "There are multiple duplicates in the data which we do not want to have in the final data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe and find duplicate rows\n",
    "genres_dedup_frames = {}\n",
    "for key in  podcast_genre_dict:\n",
    "    \n",
    "    links = podcast_genre_dict[key]['links']\n",
    "    ids = podcast_genre_dict[key]['ids']\n",
    "    \n",
    "    frame = pd.DataFrame([pd.Series(ids), pd.Series(links)]).T\n",
    "    frame.columns = ['id', 'link']\n",
    "    \n",
    "    frame_dedup = frame.drop_duplicates()\n",
    "    \n",
    "    genres_dedup_frames[key] = frame_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many podcasts were filtered out \n",
    "dedup_genre_numbers = [genres_dedup_frames[key].shape[0] for key in genres_dedup_frames]\n",
    "sum(genre_numbers) - sum(dedup_genre_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is left?\n",
    "sum(dedup_genre_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_genre_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our work as pickle file\n",
    "with open('dedup_podcasts.pickle', 'wb') as f:\n",
    "    pickle.dump(genres_dedup_frames, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get more information from iTunes\n",
    "So we have list of unique identifiers/links of podcasts. We will now crawl through every podcast page and extract addtional info. We are only interested in \"active\" podcasts. A podcast is defined as active (by me) if at least one episode has been published since January 1 2019. We use the fact that every podcast page shows the latest episode on top. Scraping the podcast information takes really long, therefore we focus here on a very small subset of the data in the True Crime genre. The rest of the scraping will be done in a simple python script that then can run for several days in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_frame = genres_dedup_frames['True Crime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casts = {}\n",
    "pc_artist_names = {}\n",
    "for index, row in genre_frame.iterrows():\n",
    "    link = row['link']\n",
    "    pc_id = row['id']\n",
    "    \n",
    "    # get\n",
    "    pc_page = requests.get(link)\n",
    "    soup_pc = BeautifulSoup(pc_page.content, 'html.parser')\n",
    "    \n",
    "    # first check if an episode was published in 2019 or later\n",
    "    # information is hidden in div third div (refers to latest episode)\n",
    "    # there are some discrepancies of episodes listed in the overview page and entries\n",
    "    # that have actually pages\n",
    "    try:\n",
    "        div_class_of_interest = soup_pc.find_all('div', class_ = \"l-row\")[2]\n",
    "\n",
    "        latest_year = int(div_class_of_interest.div.time['datetime'].split(\"-\")[0])\n",
    "\n",
    "        if latest_year >= 2019:\n",
    "            \n",
    "            # find genre, artist and artis id\n",
    "            div_class_of_interest = soup_pc.find_all('div', class_ = \"l-column small-7 medium-12 small-valign-top\")\n",
    "            \n",
    "            # not all artists have an id and/or a artist page\n",
    "            try:    \n",
    "                artist_string = div_class_of_interest[0].h1.a.contents\n",
    "                artist_string = ' '.join(artist_string[0].split())\n",
    "                artist_link = div_class_of_interest[0].h1.a['href']\n",
    "                \n",
    "                try:\n",
    "                    artist_id = artist_link.split(\"/\")[6]\n",
    "                except:\n",
    "                    artist_id = artist_link.split(\"/\")[5]\n",
    "                \n",
    "                # add it to artist dict\n",
    "                artist_names[artist_id] = artist_string\n",
    "                artist_links[artist_id] = artist_link\n",
    "                \n",
    "                # add artist name to podcast dict\n",
    "                \n",
    "            except:\n",
    "                # only get artist name to store in podcast names dict\n",
    "                artist_string = div_class_of_interest[0].h1.contents[1].contents\n",
    "                artist_string = ' '.join(artist_string[0].split())\n",
    "                \n",
    "                \n",
    "            # get genre\n",
    "            genre_string = div_class_of_interest[0].li.li.contents\n",
    "            genre_string = ' '.join(genre_string[0].split())\n",
    "            \n",
    "            # get title, description out of soup object and cut out unicode thingy\n",
    "            title = soup_pc.find(\"meta\", {\"name\":\"apple:title\"})['content'].replace('\\u200E', '')\n",
    "            description = soup_pc.find(\"meta\", {\"name\":\"apple:description\"})['content'].replace('\\u200E', '')\n",
    "            \n",
    "            \n",
    "            casts[pc_id] = np.array([pc_id, title, description, link, artist_string, genre_string])\n",
    "            \n",
    "            \n",
    "    except:\n",
    "        print(link)\n",
    "        failed[pc_id] = link\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_data_frame = pd.DataFrame([casts[key] for key in casts.keys()])\n",
    "genre_data_frame.columns = ['itunes_id', 'name', 'description', 'itunes_link', 'artist_name', 'genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column rss_feed consisting only of NAs to frame to retrieve them later\n",
    "genre_data_frame[\"rss_feed\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv file\n",
    "genre_data_frame.to_csv(\"true_crime.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the data into MongoDB Atlas database\n",
    "In the next step we upload the data into an MongoDB data base hosted by MongoDB Atlas. At this point the free tier cluster is sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load login credentials from different place for obvious reasons\n",
    "mongo_db_login_string = pickle.load( open( 'mongo_db_login_string.pickle', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(mongo_db_login_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new database named podcasts\n",
    "db = client.podcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new collection named podcast to insert our table data into\n",
    "podcasts = db.podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to json and insert data into the database\n",
    "podcasts_json = json.loads(genre_data_frame.T.to_json()).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcasts.insert_many(podcasts_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spotify_env] *",
   "language": "python",
   "name": "conda-env-spotify_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
